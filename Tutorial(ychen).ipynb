{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Tutorial: ML Model Deployment on AWS</center> \n",
    "<center>By: You Chen</center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For details about AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html  \n",
    "For details about AWS Lambda: https://docs.aws.amazon.com/en_us/lambda/?id=docs_gateway  \n",
    "For details about deploying Lambda in JAVA: https://docs.aws.amazon.com/lambda/latest/dg/lambda-java.html\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Building a deployment package with Maven</h5>\n",
    "\n",
    "<br>Apache Maven is a software project management and comprehension tool. Based on the concept of a project object model (POM), Maven can manage a project's build, reporting and documentation from a central piece of information. For detail about how to set up **Maven** : http://maven.apache.org/.</br> \n",
    "\n",
    "<br> A deployment package is a ZIP(JAR) archive that contains your compiled function code and dependencies. You can upload the package directly to Lambda, or you can use an Amazon S3 bucket, and then upload it to Lambda. If the deployment package is larger than 50 MB, you must use Amazon S3.  \n",
    "AWS Lambda provides the following libraries for Java functions:</br>\n",
    "\n",
    "* [<strong>com.amazonaws:aws-lambda-java-core (required)</strong>](https://github.com/aws/aws-lambda-java-libs/tree/master/aws-lambda-java-core) – Defines handler method interfaces and the context object that the runtime passes to the handler. If you define your own input types, this is the only library you need.\n",
    "\n",
    "* [<strong>com.amazonaws:aws-lambda-java-events</strong>](https://github.com/aws/aws-lambda-java-libs/tree/master/aws-lambda-java-events) – Input types for events from services that invoke Lambda functions.\n",
    "\n",
    "* [<strong>com.amazonaws:aws-lambda-java-log4j2</strong>](https://github.com/aws/aws-lambda-java-libs/tree/master/aws-lambda-java-log4j2) – An appender library for Log4j 2 that you can use to add the request ID for the current invocation to your function logs.\n",
    "\n",
    "These libraries are available through [Maven central repository](https://search.maven.org/search?q=g:com.amazonaws). Add them to your build definition ( _pom.xml_ ) as follows.\n",
    "```xml\n",
    "  <dependencies>\n",
    "    <dependency>\n",
    "      <groupId>com.amazonaws</groupId>\n",
    "      <artifactId>aws-lambda-java-core</artifactId>\n",
    "      <version>X.X.X</version>\n",
    "    </dependency>\n",
    "    <dependency>\n",
    "      <groupId>com.amazonaws</groupId>\n",
    "      <artifactId>aws-lambda-java-events</artifactId>\n",
    "      <version>X.X.X</version>\n",
    "    </dependency>\n",
    "    <dependency>\n",
    "      <groupId>com.amazonaws</groupId>\n",
    "      <artifactId>aws-lambda-java-log4j2</artifactId>\n",
    "      <version>X.X.X</version>\n",
    "    </dependency>\n",
    "  </dependencies>\n",
    "```\n",
    "\n",
    "<br>Other dependencies (required).</br>\n",
    "``` xml\n",
    "<dependencies>\n",
    "\t\t<dependency>\n",
    "\t\t\t<groupId>com.google.code.gson</groupId>\n",
    "\t\t\t<artifactId>gson</artifactId>\n",
    "\t\t\t<version>X.X.X</version>\n",
    "\t\t</dependency>\n",
    "        <dependency>\n",
    "            <groupId>ai.h2o</groupId>\n",
    "            <artifactId>h2o-genmodel</artifactId>\n",
    "            <version>X.X.X</version>\n",
    "        </dependency>\n",
    "</dependencies>\n",
    "```\n",
    "<br>Use the [Maven Shade plugin](https://maven.apache.org/plugins/maven-shade-plugin/). The plugin creates a JAR file that contains the compiled function code and all of its dependencies.</br>\n",
    "\n",
    "``` xml\n",
    "    <plugins>\n",
    "        <plugin>\n",
    "        <groupId>org.apache.maven.plugins</groupId>\n",
    "        <artifactId>maven-shade-plugin</artifactId>\n",
    "        <version>3.2.2</version>\n",
    "        <configuration>\n",
    "          <createDependencyReducedPom>false</createDependencyReducedPom>\n",
    "        </configuration>\n",
    "        <executions>\n",
    "          <execution>\n",
    "            <phase>package</phase>\n",
    "            <goals>\n",
    "              <goal>shade</goal>\n",
    "            </goals>\n",
    "          </execution>\n",
    "        </executions>\n",
    "      </plugin>\n",
    "    </plugins>\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>ML Model Project structure</h5>\n",
    "<br>This model lambda function can take a json string as input, the json string (RequestList) is a list of flight's info. It will generate H2O RowData, and setup the model, feed the RowData into generated model and calculate the prediction value, put the resultant prediction back into RowData, and return it as RequestList.</br>\n",
    "\n",
    "![Project1](https://github.com/AnnaChenU/AWS-lambda-MLmodel/raw/AnnaChenU-patch-imgs/mvn_struct.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` java\n",
    "GrossVolumeModel     // package (namespace), can be any cutomized name\n",
    "    GBM_4_AutoML_*   // the H2O output POJO model file (can be a different name)\n",
    "    Predictor        // Java class, configure ML model, provide predict function to update data in RequestList\n",
    "    RequestList      // Java class, as input and also output of handlerRequest method\n",
    "    VolumeHandler    // class implement com.amazonaws.services.lambda.runtime.RequestHandler (lambda function)\n",
    "\n",
    "pom.xml          // project configuration file\n",
    "```\n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictor.java\n",
    "```java\n",
    "package GrossVolumeModel;\n",
    "\n",
    "import hex.genmodel.GenModel;\n",
    "import hex.genmodel.easy.EasyPredictModelWrapper;\n",
    "import hex.genmodel.easy.RowData;\n",
    "import hex.genmodel.easy.exception.PredictException;\n",
    "import hex.genmodel.easy.prediction.RegressionModelPrediction;\n",
    "\n",
    "\n",
    "import java.lang.reflect.Method;\n",
    "import java.util.List;\n",
    "\n",
    "public class Predictor {\n",
    "    private RequestList requestList;\n",
    "    EasyPredictModelWrapper model;\n",
    "\n",
    "    public Predictor(RequestList requestList) {\n",
    "        this.requestList = requestList;\n",
    "\n",
    "        // Should better use 'Reflection' to instantiate model object in future......\n",
    "        GenModel rawModel = new GBM_4_AutoML_20200414_192014();  \n",
    "        this.model = new EasyPredictModelWrapper(new EasyPredictModelWrapper.Config()\n",
    "                .setModel(rawModel).setConvertUnknownCategoricalLevelsToNa(true));\n",
    "    }\n",
    "\n",
    "    public EasyPredictModelWrapper getModel() {\n",
    "        return model;\n",
    "    }\n",
    "\n",
    "    public RequestList getRequestList() {\n",
    "        return requestList;\n",
    "    }\n",
    "\n",
    "    public void predictVolume() {\n",
    "        RowData row = new RowData();\n",
    "        List<RowData> inputs = this.requestList.getRequestItem();\n",
    "        for (RowData input : inputs) {\n",
    "            try {\n",
    "                RegressionModelPrediction p = model.predictRegression(input);\n",
    "                input.put(\"finalVolume_pred\", \"\" + p.value);   // should have a ConfigurationManager module in future...\n",
    "            } catch (PredictException e) {\n",
    "                e.printStackTrace();\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RequestList.java\n",
    "```java\n",
    "package GrossVolumeModel;\n",
    "import java.util.List;\n",
    "import com.google.gson.annotations.Expose;\n",
    "import com.google.gson.annotations.SerializedName;\n",
    "import hex.genmodel.easy.RowData;\n",
    "\n",
    "public class RequestList {\n",
    "    @SerializedName(\"requestItem\")\n",
    "    @Expose\n",
    "    private List<RowData> requestItem = null;\n",
    "\n",
    "\n",
    "    public RequestList() { }\n",
    "\n",
    "    /**\n",
    "     * @param requestItem\n",
    "     */\n",
    "    public RequestList(List<RowData> requestItem) {\n",
    "        super();\n",
    "        this.requestItem = requestItem;\n",
    "    }\n",
    "\n",
    "    public List<RowData> getRequestItem() {\n",
    "        return requestItem;\n",
    "    }\n",
    "\n",
    "    public void setRequestItem(List<RowData> requestItem) {\n",
    "        this.requestItem = requestItem;\n",
    "    }\n",
    "}\n",
    "```\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VolumeHandler.java\n",
    "```java\n",
    "package GrossVolumeModel;\n",
    "\n",
    "import com.amazonaws.services.lambda.runtime.Context;\n",
    "import com.amazonaws.services.lambda.runtime.RequestHandler;\n",
    "import com.amazonaws.services.lambda.runtime.LambdaLogger;\n",
    "\n",
    "import com.google.gson.Gson;\n",
    "import com.google.gson.GsonBuilder;\n",
    "import com.google.gson.FieldNamingPolicy;\n",
    "\n",
    "\n",
    "public class VolumeHandler implements RequestHandler<RequestList, RequestList> {\n",
    "    Gson gson = new GsonBuilder().setFieldNamingPolicy(FieldNamingPolicy.UPPER_CAMEL_CASE).create();\n",
    "    //private static final String modelClassName = \"GBM_4_AutoML_20200414_192014\";\n",
    "    //private static final String targetName = \"FinalVolume_predicted\";\n",
    "\n",
    "    @Override\n",
    "    public RequestList handleRequest(RequestList event, Context context) {\n",
    "        LambdaLogger logger = context.getLogger();\n",
    "        logger.log(\"EVENT: \" + gson.toJson(event));\n",
    "        logger.log(\"EVENT TYPE: \" + event.getClass().toString());\n",
    "\n",
    "        Predictor predictor = new Predictor(event);\n",
    "        predictor.predictVolume();                                            // update RequestList\n",
    "        logger.log(\"RESULT: \" + gson.toJson(predictor.getRequestList()));\n",
    "\n",
    "        return predictor.getRequestList();\n",
    "    }\n",
    "}\n",
    "```\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pom.xml\n",
    "``` xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n",
    "         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n",
    "         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n",
    "    <modelVersion>4.0.0</modelVersion>\n",
    "    <groupId>org.example</groupId>\n",
    "    <artifactId>mlModel</artifactId>\n",
    "    <packaging>jar</packaging>\n",
    "    <name>mlModel</name>\n",
    "    <version>1.0.0</version>\n",
    "\n",
    "    <properties>\n",
    "        <java.version>1.8</java.version>\n",
    "        <maven.compiler.source>1.8</maven.compiler.source>\n",
    "        <maven.compiler.target>1.8</maven.compiler.target>\n",
    "        <sl4j.version>1.6.1</sl4j.version>\n",
    "        <environment>local</environment>\n",
    "        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n",
    "        <build.version>1.0.0</build.version>\n",
    "        <maven.build.timestamp.format>yyyy-MM-dd'T'HH:mm:ss.SSS'Z'</maven.build.timestamp.format>\n",
    "        <build-number>1.0.0</build-number>\n",
    "        <maven.build.timestamp.format>yyyy-MM-dd'T'HH:mm:ss.SSS'Z'</maven.build.timestamp.format>\n",
    "    </properties>\n",
    "\n",
    "    <build>\n",
    "        <plugins>\n",
    "            <plugin>\n",
    "                <artifactId>maven-surefire-plugin</artifactId>\n",
    "                <version>2.22.2</version>\n",
    "                <configuration>\n",
    "                    <argLine>-Xmx1024m</argLine>\n",
    "                </configuration>\n",
    "            </plugin>\n",
    "            <plugin>\n",
    "                <groupId>org.apache.maven.plugins</groupId>\n",
    "                <artifactId>maven-shade-plugin</artifactId>\n",
    "                <version>3.2.2</version>\n",
    "                <configuration>\n",
    "                    <createDependencyReducedPom>false</createDependencyReducedPom>\n",
    "                </configuration>\n",
    "                <executions>\n",
    "                    <execution>\n",
    "                        <phase>package</phase>\n",
    "                        <goals>\n",
    "                            <goal>shade</goal>\n",
    "                        </goals>\n",
    "                        <configuration>\n",
    "                            <transformers>\n",
    "                                <transformer implementation=\"com.github.edwgiz.maven_shade_plugin.log4j2_cache_transformer.PluginsCacheFileTransformer\">\n",
    "                                </transformer>\n",
    "                            </transformers>\n",
    "                        </configuration>\n",
    "                    </execution>\n",
    "                </executions>\n",
    "                <dependencies>\n",
    "                    <dependency>\n",
    "                        <groupId>com.github.edwgiz</groupId>\n",
    "                        <artifactId>maven-shade-plugin.log4j2-cachefile-transformer</artifactId>\n",
    "                        <version>2.13.0</version>\n",
    "                    </dependency>\n",
    "                </dependencies>\n",
    "            </plugin>\n",
    "            <plugin>\n",
    "                <groupId>org.apache.maven.plugins</groupId>\n",
    "                <artifactId>maven-compiler-plugin</artifactId>\n",
    "                <version>3.8.1</version>\n",
    "                <configuration>\n",
    "                    <source>1.8</source>\n",
    "                    <target>1.8</target>\n",
    "                </configuration>\n",
    "            </plugin>\n",
    "\n",
    "            <plugin>\n",
    "                <groupId>org.apache.maven.plugins</groupId>\n",
    "                <artifactId>maven-jar-plugin</artifactId>\n",
    "            </plugin>\n",
    "        </plugins>\n",
    "\n",
    "        <extensions>\n",
    "            <extension>\n",
    "                <groupId>org.springframework.build</groupId>\n",
    "                <artifactId>aws-maven</artifactId>\n",
    "                <version>5.0.0.RELEASE</version>\n",
    "            </extension>\n",
    "        </extensions>\n",
    "    </build>\n",
    "\n",
    "    <dependencies>\n",
    "        <!-- https://mvnrepository.com/artifact/com.amazonaws/aws-lambda-java-core -->\n",
    "        <dependency>\n",
    "            <groupId>com.amazonaws</groupId>\n",
    "            <artifactId>aws-lambda-java-core</artifactId>\n",
    "            <version>1.2.0</version>\n",
    "        </dependency>\n",
    "        <!-- https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-s3 -->\n",
    "        <dependency>\n",
    "            <groupId>com.amazonaws</groupId>\n",
    "            <artifactId>aws-lambda-java-log4j2</artifactId>\n",
    "            <version>1.1.0</version>\n",
    "        </dependency>\n",
    "        <!-- https://mvnrepository.com/artifact/com.amazonaws/aws-lambda-java-events -->\n",
    "        <dependency>\n",
    "            <groupId>com.amazonaws</groupId>\n",
    "            <artifactId>aws-lambda-java-events</artifactId>\n",
    "            <version>2.2.7</version>\n",
    "        </dependency>\n",
    "\n",
    "        <!-- https://mvnrepository.com/artifact/com.google.code.gson/gson -->\n",
    "\t\t<dependency>\n",
    "\t\t\t<groupId>com.google.code.gson</groupId>\n",
    "\t\t\t<artifactId>gson</artifactId>\n",
    "\t\t\t<version>2.8.6</version>\n",
    "\t\t</dependency>\n",
    "        <!-- https://mvnrepository.com/artifact/ai.h2o/h2o-genmodel -->\n",
    "        <dependency>\n",
    "            <groupId>ai.h2o</groupId>\n",
    "            <artifactId>h2o-genmodel</artifactId>\n",
    "            <version>3.24.0.2</version>\n",
    "        </dependency>\n",
    "    </dependencies>\n",
    "</project>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h5>Deploy ML model lambda function and setup API Gateway trigger.</h5>  \n",
    "\n",
    "\n",
    "_<strong>1.Deploy on AWS website:</strong>_\n",
    "<ul>\n",
    "\n",
    "* Run <code><strong>mvn package</strong></code> command at the project root folder (_pom.xml_ should also be in this folder) this command will build jar file in <code>target</code> folder.\n",
    "* Go to AWS S3 website\n",
    "    * <strong>Create bucket</strong>, upload the */ mlMode / target / <code>mlModel-1.0.0.jar</code> file\n",
    "* Go to AWS Lambda website\n",
    "    * <strong>Create function</strong>, set runtime as Java 8, create or link execution role (CloudWatch recommanded)\n",
    "    * Choose <strong>Action -> Upload a file from S3 bucket</strong>, then paste your model jar file S3 link URL\n",
    "    * Choose <strong>Add trigger</strong>, select <strong>API Gateway</strong>, add your API (API Gateway supports two types of RESTful APIs: HTTP APIs and REST APIs)\n",
    "* Test configuration:  (RequestList object) all keys should begin with an <strong>uppercase</strong> letter except 'id'. Any keys which are not contained in model POJO will be <strong>ignored</strong> during making prediction.\n",
    "    ```json\n",
    "{\n",
    "  \"requestItem\": [\n",
    "    {\n",
    "      \"id\": \"1\",\n",
    "      \"BookedWeight\": \"3000.0\",\n",
    "      \"BookedVolume\": 10.00,\n",
    "      \"Hamster\": \"小土豆(˘•ω•˘)\",      // this will be ignored during prediction\n",
    "      ...\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"2\",\n",
    "      \"BookedWeight\": \"2020.0\",\n",
    "      \"BookedVolume\": 7.11,\n",
    "      ...\n",
    "    }]\n",
    "}\n",
    "    ```\n",
    " </ul>\n",
    " \n",
    "Setup API Gateway:\n",
    "<ul>\n",
    "    \n",
    "* Go to AWS API Gateway website\n",
    "    * <strong>Build HTTP API or REST API</strong>, click <strong>Actions</strong> to create resource or method, finally deploy it\n",
    "</ul>\n",
    "\n",
    "To call API locally, you need API key and API endpoint. Those can be found under your API Gateway trigger details in your lambda function webpage.\n",
    " \n",
    " \n",
    "<br></br>\n",
    " \n",
    "_<strong>2.Deploy function by AWS CLI Lambda API (recommanded):</strong>_\n",
    "\n",
    "<br>Intall <code><strong>AWS CLI</strong></code> first: [AWS Command Line Interface](https://aws.amazon.com/cli/)</br>\n",
    "<br>Check AWS documents section: [Uploading a deployment package with the Lambda API](https://docs.aws.amazon.com/lambda/latest/dg/java-package.html#java-package-maven)</br>\n",
    "<br>Updates a Lambda function's code: The function's code is locked when you publish a version. You can't modify the code of a published version, only the unpublished version. [Document](https://docs.aws.amazon.com/lambda/latest/dg/API_UpdateFunctionCode.html)</br>  \n",
    " \n",
    " In future, write cmd script to update function with aws cli, as the last step of training pipline. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>FileParser project structure</h4>\n",
    "<br>This lambda function is triggered when new input csv files uploaded to specific source bucket, and it will fecth the csv file, read the input stream, convert input stream to RequestList object, then invoke model lambda function while send the RequestList in the invocation payload, get the response and save the result into another csv file, finally upload this file to destination bucket.</br>\n",
    "\n",
    "![Project2](https://github.com/AnnaChenU/AWS-lambda-MLmodel/raw/AnnaChenU-patch-imgs/fileParser.PNG)\n",
    "\n",
    "\n",
    "\n",
    "``` java\n",
    "VolumeInvoker     // package (namespace), can be any cutomized name\n",
    "    FileConverter    // Java Class, can take InputStream into RequestList\n",
    "    LambdaInvoker    // Java class, configure invocation, invoke mlModel lambda and get the response Json string\n",
    "    LambdaInvokerConfiguration   // (not complete yet) Java class, \n",
    "                                 // in future can be used to configure different model as well as \n",
    "                                 // different API Gateway, S3 (based on prediction target)\n",
    "    RequestList      // Java class, as input and also output of handlerRequest method\n",
    "    S3Reader         // class implement com.amazonaws.services.lambda.runtime.RequestHandler  (lambda function)\n",
    "\n",
    "pom.xml          // project configuration file\n",
    "```\n",
    "-----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FileConverter.java\n",
    "```java\n",
    "package VolumeInvoker;\n",
    "\n",
    "import com.opencsv.CSVReader;\n",
    "import java.io.InputStream;\n",
    "import java.io.InputStreamReader;\n",
    "import java.util.HashMap;\n",
    "import java.util.Map;\n",
    "\n",
    "/*\n",
    "FileConverter can take InputStream into RequestList\n",
    " */\n",
    "public class FileConverter {\n",
    "    private InputStream objectData;\n",
    "    private RequestList requestList;\n",
    "\n",
    "    public FileConverter(InputStream objectData) {\n",
    "        this.objectData = objectData;\n",
    "        this.requestList = new RequestList();\n",
    "    }\n",
    "\n",
    "    public InputStream getObjectData() {\n",
    "        return objectData;\n",
    "    }\n",
    "\n",
    "    public RequestList getRequestList() {\n",
    "        return requestList;\n",
    "    }\n",
    "\n",
    "    public void setObjectData(InputStream objectData) {\n",
    "        this.objectData = objectData;\n",
    "    }\n",
    "\n",
    "    // convert data in csv, to RequestList\n",
    "    public void convertCSV2List() {\n",
    "        try {\n",
    "            Map<Integer, String> header = new HashMap<>();\n",
    "\n",
    "            InputStreamReader isr = new InputStreamReader(this.objectData, \"UTF-8\");\n",
    "            CSVReader csvReader = new CSVReader(isr);\n",
    "\n",
    "            String[] line = csvReader.readNext();                       // file header (column's names)\n",
    "            for (int i = 0; i < line.length; i++) {\n",
    "                header.put(i, line[i]);                                 // map column name to index\n",
    "            }\n",
    "\n",
    "            while ((line = csvReader.readNext()) != null) {\n",
    "                Map<String, Object> row = new HashMap<>();              // {\"Column_name\" : \"data\"}\n",
    "\n",
    "                for (int i = 0; i < line.length; i++) {\n",
    "                    String key = header.get(i).substring(0,1).toUpperCase() + header.get(i).substring(1);\n",
    "                    row.put(key, line[i]);                      // map data to associated column name\n",
    "                }\n",
    "                this.requestList.getRequestItem().add(row);\n",
    "            }\n",
    "\n",
    "            csvReader.close();\n",
    "\n",
    "        } catch (Exception e) {\n",
    "            e.printStackTrace();\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LambdaInvoker.java\n",
    "```java\n",
    "package VolumeInvoker;\n",
    "\n",
    "import com.amazonaws.AmazonClientException;\n",
    "import com.amazonaws.services.lambda.AWSLambda;\n",
    "import com.amazonaws.services.lambda.AWSLambdaClientBuilder;\n",
    "import com.amazonaws.services.lambda.model.InvocationType;\n",
    "import com.amazonaws.services.lambda.model.InvokeRequest;\n",
    "import com.amazonaws.services.lambda.model.InvokeResult;\n",
    "import com.amazonaws.services.lambda.runtime.LambdaLogger;\n",
    "import com.amazonaws.services.lambda.runtime.events.S3Event;\n",
    "import com.amazonaws.services.s3.AmazonS3;\n",
    "import com.amazonaws.services.s3.AmazonS3ClientBuilder;\n",
    "import com.amazonaws.services.s3.event.S3EventNotification;\n",
    "import com.amazonaws.services.s3.model.GetObjectRequest;\n",
    "import com.amazonaws.services.s3.model.S3Object;\n",
    "import com.google.gson.Gson;\n",
    "\n",
    "import java.io.InputStream;\n",
    "import java.nio.charset.Charset;\n",
    "\n",
    "public class LambdaInvoker {\n",
    "    private String response;\n",
    "    LambdaInvokerConfiguration config = new LambdaInvokerConfiguration();\n",
    "    Gson gson = new Gson();\n",
    "\n",
    "    public LambdaInvoker() {\n",
    "        response = null;\n",
    "    }\n",
    "\n",
    "    public String getResponse() {\n",
    "        return response;\n",
    "    }\n",
    "\n",
    "    public void setResponse(String response) {\n",
    "        this.response = response;\n",
    "    }\n",
    "\n",
    "    public LambdaInvokerConfiguration getConfig() {\n",
    "        return config;\n",
    "    }\n",
    "\n",
    "    public void setConfig(LambdaInvokerConfiguration config) {\n",
    "        this.config = config;\n",
    "    }\n",
    "\n",
    "    // Fetch the data from newly updated file in S3,\n",
    "    // parse the data and generate RequestList object,\n",
    "    // Invoke ML lambda with serialized RequestList,\n",
    "    public String getResponse(S3Event s3event, LambdaLogger logger) {\n",
    "        // get the final S3 event\n",
    "        S3EventNotification.S3EventNotificationRecord record = s3event.getRecords().get(0);                              \n",
    "        String srcBucket = record.getS3().getBucket().getName();\n",
    "        String srcKey = record.getS3().getObject().getKey().replace('+', ' ');       // file name\n",
    "\n",
    "        logger.log(\"srcBucket: \" + srcBucket);\n",
    "        logger.log(\"srcKey (file): \" + srcKey);\n",
    "\n",
    "        if (srcBucket.equals(config.SRC_BUCKET)) {\n",
    "            try {\n",
    "                AmazonS3 s3Client = AmazonS3ClientBuilder.standard().build();\n",
    "                \n",
    "                // get object file using source bucket and srcKey name\n",
    "                S3Object s3Object = s3Client.getObject(new GetObjectRequest(srcBucket, srcKey));    \n",
    "                InputStream objectData = s3Object.getObjectContent();     // get content of the file\n",
    "\n",
    "                logger.log(\"Lambda function s3Reader is invoked:\" + s3event.toJson());\n",
    "                try {\n",
    "                    FileConverter converter = new FileConverter(objectData);\n",
    "                    \n",
    "                    // generate ML lambda function input, call convertCSV2List() method\n",
    "                    converter.convertCSV2List();                          \n",
    "                    \n",
    "                    RequestList requestItems = converter.getRequestList();\n",
    "\n",
    "                    logger.log(\"REQUEST ITEM: \" + gson.toJson(requestItems));\n",
    "\n",
    "                    // invoke ML lambda function, call invokeLambda() method\n",
    "                    this.response = invokeLambda(gson.toJson(requestItems));   \n",
    "\n",
    "                    logger.log(\"RESPONSE ITEM: \" + response);\n",
    "                } catch (Exception e) {\n",
    "                    logger.log(e.getMessage());\n",
    "                    e.printStackTrace();\n",
    "                }\n",
    "            } catch (AmazonClientException e) {\n",
    "                logger.log(e.getLocalizedMessage());\n",
    "                e.printStackTrace();\n",
    "            }\n",
    "        }\n",
    "        return this.response;      // response is serialized RequestList string\n",
    "    }\n",
    "\n",
    "\n",
    "    /** Invoke ML model lambda function\n",
    "     * @param payload = gson.toJson(requestItems);\n",
    "     * @return return string is a String has same format with RequestList which contain the prediction result \n",
    "     */\n",
    "    private String invokeLambda(String payload) {\n",
    "\n",
    "        InvokeRequest lmbRequest = new InvokeRequest()\n",
    "                .withFunctionName(config.FUNCTION_NAME)\n",
    "                .withPayload(payload)\n",
    "                .withInvocationType(InvocationType.RequestResponse);\n",
    "\n",
    "        AWSLambda lambda = AWSLambdaClientBuilder.standard().build();\n",
    "\n",
    "        InvokeResult lmbResult = lambda.invoke(lmbRequest);\n",
    "\n",
    "        return new String(lmbResult.getPayload().array(), Charset.forName(\"UTF-8\"));\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LambdaInvokerConfiguration.java\n",
    "```java\n",
    "package VolumeInvoker;\n",
    "\n",
    "// this class is incomplete...\n",
    "// I write this way just for convenience...  \n",
    "\n",
    "public class LambdaInvokerConfiguration {\n",
    "\n",
    "    public final String REGION = \"US_WEST_2\";\n",
    "\n",
    "    public final String AWS_ACCESS_KEY_ID = \"\";\n",
    "\n",
    "    public final String AWS_SECRET_ACCESS_KEY = \"\";\n",
    "\n",
    "    public final String FUNCTION_NAME = \"arn:aws:lambda:us-west-2:460908697650:function:GrossVolumePredict\";\n",
    "\n",
    "    public final String DST_BUCKET = \"cargo.ml.resource\";\n",
    "\n",
    "    public final String SRC_BUCKET = \"cargo.ml.resource\";\n",
    "\n",
    "    public final String API_ENDPOINT = \"https://h6ofj21659.execute-api.us-west-2.amazonaws.com/Dev/GrossVolumePredict\";\n",
    "\n",
    "    public final String API_KEY = \"hwPdf0Leum9pJce3g5wSZ9rmjptbEpI091hwsFDX\";\n",
    "\n",
    "    public final String volume_outputKey = \"_Volume_Prediction.csv\";   // object file to upload\n",
    "\n",
    "}\n",
    "\n",
    "```\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RequestList.java\n",
    "(same with the former one)  \n",
    "```java\n",
    "```\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3Reader.java\n",
    "```java\n",
    "package VolumeInvoker;\n",
    "\n",
    "import com.amazonaws.services.lambda.runtime.RequestHandler;\n",
    "import com.amazonaws.services.s3.AmazonS3;\n",
    "import com.amazonaws.services.s3.AmazonS3ClientBuilder;\n",
    "import com.amazonaws.services.s3.model.ObjectMetadata;\n",
    "\n",
    "\n",
    "import com.amazonaws.services.lambda.runtime.LambdaLogger;\n",
    "import com.amazonaws.services.lambda.runtime.events.S3Event;\n",
    "import com.amazonaws.services.lambda.runtime.Context;\n",
    "\n",
    "import com.google.gson.Gson;\n",
    "import com.opencsv.*;\n",
    "import org.apache.http.HttpResponse;\n",
    "import org.apache.http.client.HttpClient;\n",
    "import org.apache.http.client.methods.HttpPost;\n",
    "import org.apache.http.entity.StringEntity;\n",
    "import org.apache.http.impl.client.HttpClientBuilder;\n",
    "import org.joda.time.DateTime;\n",
    "\n",
    "import java.io.*;\n",
    "import java.nio.charset.Charset;\n",
    "import java.nio.charset.StandardCharsets;\n",
    "import java.util.*;\n",
    "\n",
    "public class S3Reader implements RequestHandler<S3Event, String> {\n",
    "    LambdaInvoker invoker = new LambdaInvoker();\n",
    "\n",
    "    @Override\n",
    "    public String handleRequest(S3Event s3event, Context context) {\n",
    "        LambdaLogger logger = context.getLogger();\n",
    "\n",
    "        // parse input file, invoke model lambda with serialized payload, call getResponse() method\n",
    "        String textToUpload = invoker.getResponse(s3event, logger);  \n",
    "        logger.log(\"Response \" + textToUpload);\n",
    "\n",
    "        ObjectMetadata meta = null;\n",
    "        InputStream is = null;\n",
    "        byte[] bytes = textToUpload.getBytes(StandardCharsets.UTF_8);\n",
    "        is = new ByteArrayInputStream(bytes);\n",
    "\n",
    "        //set meta information about text to be uploaded\n",
    "        meta = new ObjectMetadata();\n",
    "        meta.setContentLength(bytes.length);\n",
    "        meta.setContentType(\"text/csv\");\n",
    "\n",
    "        // upload the file by specifying destination bucket name, file name, \n",
    "        // input stream having content to be uploaded along with meta information\n",
    "        AmazonS3 s3Client = AmazonS3ClientBuilder.standard().build();\n",
    "        s3Client.putObject(invoker.config.DST_BUCKET, \n",
    "                           DateTime.now().toString() + invoker.config.volume_outputKey,\n",
    "                           is, \n",
    "                           meta); \n",
    "        logger.log(\"File uploaded.\");\n",
    "\n",
    "        return \"200 OK\";\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pom.xml\n",
    "``` xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n",
    "         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n",
    "         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n",
    "    <modelVersion>4.0.0</modelVersion>\n",
    "    <groupId>com.rms</groupId>\n",
    "    <artifactId>predictionInvoker</artifactId>\n",
    "    <packaging>jar</packaging>\n",
    "    <name>ModelInvoker</name>\n",
    "    <version>1.0.0</version>\n",
    "\n",
    "    <properties>\n",
    "        <java.version>1.8</java.version>\n",
    "        <sl4j.version>1.6.1</sl4j.version>\n",
    "        <environment>local</environment>\n",
    "        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n",
    "        <build.version>1.0.0</build.version>\n",
    "        <maven.build.timestamp.format>yyyy-MM-dd'T'HH:mm:ss.SSS'Z'</maven.build.timestamp.format>\n",
    "        <build-number>1.0.0</build-number>\n",
    "        <maven.build.timestamp.format>yyyy-MM-dd'T'HH:mm:ss.SSS'Z'</maven.build.timestamp.format>\n",
    "    </properties>\n",
    "\n",
    "    <build>\n",
    "        <plugins>\n",
    "            <plugin>\n",
    "                <artifactId>maven-surefire-plugin</artifactId>\n",
    "                <version>2.22.2</version>\n",
    "            </plugin>\n",
    "            <plugin>\n",
    "                <groupId>org.apache.maven.plugins</groupId>\n",
    "                <artifactId>maven-shade-plugin</artifactId>\n",
    "                <version>3.2.2</version>\n",
    "                <configuration>\n",
    "                    <createDependencyReducedPom>false</createDependencyReducedPom>\n",
    "                </configuration>\n",
    "                <executions>\n",
    "                    <execution>\n",
    "                        <phase>package</phase>\n",
    "                        <goals>\n",
    "                            <goal>shade</goal>\n",
    "                        </goals>\n",
    "                        <configuration>\n",
    "                            <transformers>\n",
    "                                <transformer implementation=\"com.github.edwgiz.maven_shade_plugin.log4j2_cache_transformer.PluginsCacheFileTransformer\">\n",
    "                                </transformer>\n",
    "                            </transformers>\n",
    "                        </configuration>\n",
    "                    </execution>\n",
    "                </executions>\n",
    "                <dependencies>\n",
    "                    <dependency>\n",
    "                        <groupId>com.github.edwgiz</groupId>\n",
    "                        <artifactId>maven-shade-plugin.log4j2-cachefile-transformer</artifactId>\n",
    "                        <version>2.13.0</version>\n",
    "                    </dependency>\n",
    "                </dependencies>\n",
    "            </plugin>\n",
    "            <plugin>\n",
    "                <groupId>org.apache.maven.plugins</groupId>\n",
    "                <artifactId>maven-compiler-plugin</artifactId>\n",
    "                <version>3.8.1</version>\n",
    "                <configuration>\n",
    "                    <source>8</source>\n",
    "                    <target>8</target>\n",
    "                </configuration>\n",
    "            </plugin>\n",
    "\n",
    "            <plugin>\n",
    "                <groupId>org.apache.maven.plugins</groupId>\n",
    "                <artifactId>maven-jar-plugin</artifactId>\n",
    "            </plugin>\n",
    "        </plugins>\n",
    "\n",
    "        <extensions>\n",
    "            <extension>\n",
    "                <groupId>org.springframework.build</groupId>\n",
    "                <artifactId>aws-maven</artifactId>\n",
    "                <version>5.0.0.RELEASE</version>\n",
    "            </extension>\n",
    "        </extensions>\n",
    "    </build>\n",
    "\n",
    "    <dependencies>\n",
    "        <!-- https://mvnrepository.com/artifact/com.amazonaws/aws-lambda-java-core -->\n",
    "        <dependency>\n",
    "            <groupId>com.amazonaws</groupId>\n",
    "            <artifactId>aws-lambda-java-core</artifactId>\n",
    "            <version>1.2.0</version>\n",
    "        </dependency>\n",
    "        <!-- https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-s3 -->\n",
    "        <dependency>\n",
    "            <groupId>com.amazonaws</groupId>\n",
    "            <artifactId>aws-lambda-java-log4j2</artifactId>\n",
    "            <version>1.1.0</version>\n",
    "        </dependency>\n",
    "        <!-- https://mvnrepository.com/artifact/com.amazonaws/aws-lambda-java-events -->\n",
    "        <dependency>\n",
    "            <groupId>com.amazonaws</groupId>\n",
    "            <artifactId>aws-lambda-java-events</artifactId>\n",
    "            <version>2.2.7</version>\n",
    "        </dependency>\n",
    "        <!-- https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-s3 -->\n",
    "        <dependency>\n",
    "            <groupId>com.amazonaws</groupId>\n",
    "            <artifactId>aws-java-sdk-s3</artifactId>\n",
    "            <version>1.11.578</version>\n",
    "        </dependency>\n",
    "        <!-- https://mvnrepository.com/artifact/com.opencsv/opencsv -->\n",
    "        <dependency>\n",
    "            <groupId>com.opencsv</groupId>\n",
    "            <artifactId>opencsv</artifactId>\n",
    "            <version>5.1</version>\n",
    "        </dependency>\n",
    "        <dependency>\n",
    "            <groupId>com.amazonaws</groupId>\n",
    "            <artifactId>aws-java-sdk-lambda</artifactId>\n",
    "            <version>1.11.24</version>\n",
    "        </dependency>\n",
    "        <dependency>\n",
    "            <groupId>com.amazonaws</groupId>\n",
    "            <artifactId>aws-java-sdk-s3</artifactId>\n",
    "            <version>1.11.779</version>\n",
    "        </dependency>\n",
    "        <dependency>\n",
    "            <groupId>com.google.code.gson</groupId>\n",
    "            <artifactId>gson</artifactId>\n",
    "            <version>2.8.6</version>\n",
    "        </dependency>\n",
    "    </dependencies>\n",
    "</project>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Deploy FileParser lambda function and setup S3 event trigger.</h5>  \n",
    "\n",
    "* Same as above (execution role needs <code>CloudWatch and S3FullAccess policy</code> (full access is just for covenient...TBD))\n",
    "* Add S3 event trigger:\n",
    "    * Choose <strong>Add trigger</strong>, select <strong>S3</strong>, under <code>Event Type</code> select <strong>All object create events / PUT</strong>, customize <code>Prefix</code> and <code>Suffix</code> (optional). The source bucket name is required to fetch correct file, so here the bucket name should be exactly what you write in your project code\n",
    "* Add AWS Lamda destination\n",
    "    * Under <code>Destination type</code> select <strong>Lambda function</strong> and choose the destination function (here is your mlModel lambda)\n",
    " \n",
    "\n",
    "* Test configuration: refer to <code>s3-put Event template</code>.  \n",
    "\n",
    "_<strong>Also, recommand deploying function by AWS CLI Lambda API</strong>_ [Uploading a deployment package with the Lambda API](https://docs.aws.amazon.com/lambda/latest/dg/java-package.html#java-package-maven)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> The relationship between above two labmda functions: </h4>\n",
    "\n",
    "![Pipeline](https://github.com/AnnaChenU/AWS-lambda-MLmodel/raw/AnnaChenU-patch-imgs/2lambda.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Tutorial: Build Training Pipeline With AWS</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <strong>Training piple:</strong>\n",
    "* 1. Upload new training input data on specific source <code>training input S3 bucket</code>;\n",
    "\n",
    "\n",
    "* 2. <code>S3 PUT event</code> trigger <code>ML_training_ec2Trigger</code> lambda function;\n",
    "\n",
    "\n",
    "* 3. <code><strong>ML_training_ec2Trigger</strong></code> starts specific <code><strong>ec2 instance</strong></code> (Linux instance which contains your training script as well as your maven model project);\n",
    "    * Once the <code>ec2 instance</code> finished initialization, get the _IP address_\n",
    "    \n",
    "    * Invoke destination <code>MLtraining</code> lambda function with _IP address_ as payload\n",
    "    \n",
    "    * Once invoke <code>MLtraining</code> lambda, without waiting for response, return ''invocation success'' message\n",
    "\n",
    "\n",
    "* 4. <code><strong>ML_training_ec2Trigger</strong></code> invokes <code><strong>MLtraining</strong></code> lambda\n",
    "    * (This function needs a <code>paramikoPackage</code> <strong>function layer</strong> to use <code>-ssh</code> command)\n",
    "    \n",
    "    * (This function also needs a <code>private key pair</code> for ssh to connect ec2, so you need to hold your key in a private <code>key S3 bucket</code>)\n",
    "    \n",
    "    * Once this function is invoked, it will download your private key file and use <code>paramiko</code> to <code>ssh</code> connect your running <code>ec2</code> by <code>IP address</code>\n",
    "    \n",
    "    * After connection is built, it will execute command to initialize a shell session, and start execute a bash script on that ec2, then detach the current ssh session. (which make sure script will keep executing on ec2, but this lambda function just return its response and cool down.)\n",
    "    \n",
    "    * The bash script  <code><strong>deployMode.sh</strong></code> commands:\n",
    "        * a). Download new input training data file from <code>training input S3 bucket</code>;\n",
    "        \n",
    "        * b). Python run <code>*training.py</code> file with new input csv file as <code>argument</code>;\n",
    "        \n",
    "        * c). Once generated POJO file, add _\"package XX;\"_ to the first line (packege name is same with your model lambda package name);\n",
    "        \n",
    "        * d). Move POJO into your <code>*/{projectFolder}/src/main/java/{packageName}/</code> folder (is also where your lambda function locates);\n",
    "        \n",
    "        * e). Run <code>mvn clean package -f */{projectFolder}/pom.xml > {AnyWhereYouLike}/mvn_log.txt </code> (Package your lambda function project, meanwhile save the maven execution log file, checking the log file later can help you know whether it builds successfully or not.)\n",
    "        \n",
    "        * f). Run <code>aws s3 cp */{projectFolder}/target/{Lambda}.jar s3://{modelBucket}/{lambda_Name}.jar</code> (Upload lambda function jar file to your <code>model S3 bucket</code>). Also upload any files you want here.\n",
    "        \n",
    "        \n",
    "* 5. When <code><strong>deployModel.sh</strong></code> finished, some log files will be uploaded to <code>Log files S3 Bucket</code>, this <code>S3 PUT event</code> trigger <code><strong>stop_ec2</strong></code> lambda function to stop my running ec2 instance;\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pipeline](https://github.com/AnnaChenU/AWS-lambda-MLmodel/blob/AnnaChenU-patch-imgs/pipeline.PNG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following 3 lambda functions runtime needs to be <code>Python 3.6 </code>\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong><h4>ML_training_ec2Trigger Lambda</h4></strong>\n",
    "* Execution role: AmazonEC2FullAccess, AmazonS3FullAccess  (TBD) \n",
    "* Trigger:\n",
    "    * Event type: S3 ObjectCreated\n",
    "* Destination:\n",
    "    * arn: aws:lambda:us-west-2:{awsSourceAccountNumber}:function:<code>MLtraining</code>\n",
    "    \n",
    "``` json\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Id\": \"default\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Sid\": \"lambda-6dd23a55-4102-41a6-b159-9c9a08a3edda\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": \"s3.amazonaws.com\"\n",
    "      },\n",
    "      \"Action\": \"lambda:InvokeFunction\",\n",
    "      \"Resource\": \"arn:aws:lambda:us-west-2:460908697650:function:ML_training_ec2Trigger\",\n",
    "      \"Condition\": {\n",
    "        \"StringEquals\": {\n",
    "          \"AWS:SourceAccount\": \"460908697650\"\n",
    "        },\n",
    "        \"ArnLike\": {\n",
    "          \"AWS:SourceArn\": \"arn:aws:s3:::cargo.ml.training\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ec2_trigger.py (script name)\n",
    "``` python\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "\n",
    "region = 'us-west-2'\n",
    "instances = ['i-025d579ae9136aa40']       # your Linux EC2 instance id list\n",
    "\n",
    "def trigger_handler(event, context):    \n",
    "    ec2 = boto3.client('ec2', region_name=region)\n",
    "    ec2.start_instances(InstanceIds=instances)   \n",
    "    time.sleep(3.0)                       # wait for ec2 starting\n",
    "    print (\"instances: \", str(instances))\n",
    "    \n",
    "    # get IP addresses of EC2 instances\n",
    "    client = boto3.client('ec2')\n",
    "    \n",
    "    # you need to define a tag {\"Environment\": \"Dev\"} before calling this\n",
    "    instDict=client.describe_instances(Filters=[{'Name':'tag:Environment','Values':['Dev']}]) \n",
    "    print(\"Instances Dictionary: \", instDict)\n",
    "    hostList=[]\n",
    "\n",
    "    for r in instDict['Reservations']:\n",
    "        for inst in r['Instances']:\n",
    "            print (\"Instance Details: \", inst)\n",
    "            ipaddress = inst.get(u'PublicIpAddress')\n",
    "            if ipaddress is None:\n",
    "                print(\"No key.\")\n",
    "            hostList.append(inst['PublicIpAddress'])\n",
    "            \n",
    "    print(\"Host List: \", hostList)\n",
    "\n",
    "    #Invoke worker function for each IP address\n",
    "    print(\"Invoking......\")\n",
    "    client = boto3.client('lambda')\n",
    "    for host in hostList:\n",
    "        print (\"Invoking worker_function on \" + host)\n",
    "        payload='{\"IP\":\"'+ host +'\"}'\n",
    "        print(payload)\n",
    "        \n",
    "        invokeResponse=client.invoke(\n",
    "            FunctionName='MLtraining',      # destination function \n",
    "            InvocationType='Event',\n",
    "            LogType='Tail',\n",
    "            Payload=payload                 # IP address\n",
    "        )\n",
    "        print (\"response: \", invokeResponse)\n",
    "\n",
    "    return{\n",
    "        'message' : \"Trigger function finished \"\n",
    "    }\n",
    "```\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong><h4>MLtraining Lambda</h4></strong>\n",
    "\n",
    "* Execution role: AmazonEC2FullAccess, AmazonS3FullAccess  (TBD)\n",
    "* Layers:\n",
    "    * paramikoPackage (use for ssh command)\n",
    "* Trigger:\n",
    "    * None\n",
    "* Destination:\n",
    "    * None\n",
    "    \n",
    "About how to create <code>paramiko</code> function layer: https://www.linuxschoolonline.com/how-to-solve-unable-to-load-module-when-using-paramiko-package-in-a-lambda-function/  \n",
    "I choose to install <code>paramiko</code> on my Amazon Linux EC2 instance, and package it and deploy it on EC2 by AWS CLI. So you also need to install AWS CLI on your EC2 instance first. (https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html#cliv2-linux-install)  \n",
    "Also you can consider useing paramiko layer which deployed by someone else. https://github.com/jetbridge/paramiko-lambda-layer (Region \"us-west-2\")\n",
    "\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TrainingInvoker.py (script name)\n",
    "``` python\n",
    "import boto3\n",
    "import paramiko\n",
    "import sys\n",
    "\n",
    "def ec2_training_invoker(event, context):\n",
    "    print (event)  # event here is the IP address of running ec2\n",
    "    s3_client = boto3.client('s3')\n",
    "    # download private key file from secure S3 bucket\n",
    "    s3_client.download_file('ec2.invoker', 'mykey.pem', '/tmp/mykey.pem')  # (bucketName, fileName, downloadTargetName)\n",
    "\n",
    "    k = paramiko.RSAKey.from_private_key_file(\"/tmp/mykey.pem\")\n",
    "    c = paramiko.SSHClient()\n",
    "    c.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "\n",
    "    host=event['IP']\n",
    "    print(\"Connecting to \" + host)\n",
    "    c.connect( hostname = host, username = \"ec2-user\", pkey = k )\n",
    "    print(\"Connected to \" + host)\n",
    "\n",
    "    commands = [\n",
    "        # aws s3 cp s3://{scriptBucket}/deploy.sh /Application/cargoRM/dev/deployModel.sh\n",
    "        \"source ~/.bashrc\",                                 # set environment variables\n",
    "        \"nohup ./Application/cargoRM/dev/deployModel.sh &\"  # execute bash script and detach the running process\n",
    "        ]\n",
    "\n",
    "    for command in commands:\n",
    "        print(\"Executing {}\".format(command))\n",
    "        stdin , stdout, stderr = c.exec_command(command)\n",
    "        print(stdout.read())\n",
    "        print(stderr.read())\n",
    "\n",
    "    return\n",
    "    {\n",
    "        'message' : \"Script execution completed. See Cloudwatch logs for complete output\"\n",
    "    }\n",
    "    \n",
    "   ```\n",
    "   -----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong><h4>stop_ec2 Lambda</h4></strong>\n",
    "\n",
    "* Execution role: AmazonEC2FullAccess, AmazonS3FullAccess  (TBD)\n",
    "\n",
    "* Trigger:\n",
    "    * arn:aws:s3:::training.log ----ObjectCreatedByPut\n",
    "* Destination:\n",
    "    * None\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### stop_ec2_handler.py\n",
    "``` python\n",
    "import json\n",
    "import boto3 \n",
    "\n",
    "instances = ['i-025d579ae9136aa40']     # EC2 instance id list\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    ec2 = boto3.client('ec2', region_name='us-west-2')    \n",
    "    ec2.stop_instances(InstanceIds=instances)    \n",
    "    print ('stopped instances: ' + str(instances))\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('Shut down running EC2!')\n",
    "    }\n",
    "```\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Amazon Linux EC2 instance\n",
    "\n",
    "#### 1. Launch Amazon Linux EC2 instance:\n",
    "Refer to https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/launching-instance.html  \n",
    "In Step 5: _\"Add Tags\"_  \n",
    "* <code>{'Name':'tag:Environment','Values':['Dev']}</code>  (You can define it or edit it after launching.)\n",
    "\n",
    "In Step 6: _\"Configure Security Group\"_  \n",
    "* ![inbound](https://github.com/AnnaChenU/AWS-lambda-MLmodel/raw/AnnaChenU-patch-imgs/inbound.PNG)\n",
    "\n",
    "In Step 7: _\"Review Instance Launch and Select Key Pair\"_\n",
    "* Remember save your private Key Pair in a safe location. \n",
    "\n",
    "#### 2. Connect to Linux instance\n",
    "Refer to https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstances.html  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Configure EC2 instance\n",
    "Details can be found :https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Configure_Instance.html\n",
    "\n",
    "* <strong>a).</strong> Install <code><strong>python</strong></code> 3.6 on your EC2 (python 3+ needed, not python 2)\n",
    "    ``` bash\n",
    "    [ec2-user@ip-****~]$ sudo yum install python36 -y\n",
    "    ```\n",
    "    If need to make a symbolic link, refer to: https://sixbyseven.dev/how-to-install-python-3-x-on-amazon-ec2-instance/  \n",
    "    \n",
    "    Add the executable path,  <code>~/.local/bin</code>, to your <code>PATH</code> variable:\n",
    "    Find your shell's profile script in your user folder:\n",
    "    ``` bash\n",
    "    [ec2-user@ip-****~]$ ls -a ~\n",
    "    ```  \n",
    "    \n",
    "    Add an export command to your profile script (<code>.bash_profile</code>). The following example adds the path represented by <code>LOCAL_PATH</code> to the current <code>PATH</code> variable>.\n",
    "    ```shell\n",
    "    export PATH=LOCAL_PATH:$PATH\n",
    "    ```  \n",
    "    \n",
    "    Load the profile script described in the first step into your current session:\n",
    "    ```shell\n",
    "    [ec2-user@ip-****~]$ source ~/{PROFILE_SCRIPT}\n",
    "    ```\n",
    "    \n",
    "    Install and use virtualenv <strong>(optional)</strong>:\n",
    "    ``` bash\n",
    "    [ec2-user@ip-****~]$ sudo pip3 install virtualenv\n",
    "    ```  \n",
    "    \n",
    "    Create virtual environment:\n",
    "    ```shell\n",
    "    [ec2-user@ip-****~]$ virtualenv {your_project_name}\n",
    "    ```\n",
    "    \n",
    "    Activate environment:  \n",
    "    ```shell\n",
    "    [ec2-user@ip-****~]$ source {your_project_name}/bin/activate\n",
    "    ```\n",
    "    \n",
    "    To activate the virtual environment automatically when you log in, add it to the <code>~/.bashrc</code> file:\n",
    "    \n",
    "    ```shell\n",
    "    [ec2-user@ip-****~]$ echo \"source ${HOME}/{your_project_name}/env/bin/activate\" >> ${HOME}/.bashrc\n",
    "    ```\n",
    "    \n",
    "     Source the <code>~/.bashrc</code> file in your home directory to reload your environment's bash environment. Reloading automatically activates your virtual environment. The prompt reflects the change (env). This change also applies to any future SSH sessions.\n",
    "     \n",
    "     ```shell\n",
    "     [ec2-user@ip-****~]$ source ~/.bashrc\n",
    "\n",
    "     ```\n",
    "     \n",
    "     To deactivate your environment:\n",
    "    ```shell\n",
    "    [ec2-user@ip-****~]$ deactivate\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python install requiremets packages: \n",
    "    Create your requirements.txt file first:\n",
    "    ``` shell\n",
    "    [ec2-user@ip-****~]$ vim requirements.txt\n",
    "    ```  \n",
    "    \n",
    "    Insert below into your <code>requirements.txt</code> (versions are optional):\n",
    "    ```\n",
    "    appdirs==1.4.4\n",
    "    bcrypt==3.1.7\n",
    "    boto3==1.13.24\n",
    "    botocore==1.16.24\n",
    "    certifi==2020.4.5.1\n",
    "    cffi==1.14.0\n",
    "    chardet==3.0.4\n",
    "    colorama==0.4.3\n",
    "    cryptography==2.9.2\n",
    "    distlib==0.3.0\n",
    "    docutils==0.15.2\n",
    "    filelock==3.0.12\n",
    "    future==0.18.2\n",
    "    h2o==3.30.0.3\n",
    "    idna==2.9\n",
    "    importlib-metadata==1.6.0\n",
    "    importlib-resources==1.5.0\n",
    "    jmespath==0.10.0\n",
    "    joblib==0.15.1\n",
    "    numpy==1.18.4\n",
    "    paramiko==2.7.1\n",
    "    pycparser==2.20\n",
    "    PyNaCl==1.4.0\n",
    "    python-dateutil==2.8.1\n",
    "    requests==2.23.0\n",
    "    s3transfer==0.3.3\n",
    "    scikit-learn==0.23.1\n",
    "    scipy==1.4.1\n",
    "    six==1.15.0\n",
    "    tabulate==0.8.7\n",
    "    threadpoolctl==2.0.0\n",
    "    urllib3==1.25.9\n",
    "    virtualenv==20.0.21\n",
    "    zipp==3.1.0\n",
    "    ```\n",
    "    \n",
    "    Pip install all packages:\n",
    "    \n",
    "    ``` bash\n",
    "    [ec2-user@ip-****~]$ pip3 install -r requirements.txt \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <strong>b).</strong> Install <strong>JDK</strong> and <strong>Maven</strong> on EC2:\n",
    "\n",
    "    Refer to https://docs.aws.amazon.com/neptune/latest/userguide/iam-auth-connect-prerq.html\n",
    "    \n",
    "    (Can't remember details......)\n",
    "    \n",
    "##### .bash_profile\n",
    "```bash\n",
    "# .bash_profile\n",
    "\n",
    "# Get the aliases and functions\n",
    "if [ -f ~/.bashrc ]; then\n",
    "        . ~/.bashrc\n",
    "fi\n",
    "\n",
    "# User specific environment and startup programs\n",
    "\n",
    "PATH=$PATH:$HOME/.local/bin:$HOME/bin\n",
    "\n",
    "# java env variable\n",
    "\n",
    "JAVA_HOME=\"/usr/java/jdk-14.0.1\"\n",
    "\n",
    "PATH=$JAVA_HOME/bin:$PATH\n",
    "\n",
    "export PATH\n",
    "\n",
    "```  \n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### .bashrc\n",
    "\n",
    "```bash\n",
    "# .bashrc\n",
    "\n",
    "# Source global definitions\n",
    "if [ -f /etc/bashrc ]; then\n",
    "        . /etc/bashrc\n",
    "fi\n",
    "\n",
    "# User specific aliases and functions\n",
    "\n",
    "export JAVA_HOME=\"/usr/lib/jvm/java-1.8.0\"\n",
    "\n",
    "PATH=$JAVA_HOME/bin:$PATH\n",
    "\n",
    "export M2_HOME=/usr/share/apache-maven/apache-maven-3.6.3\n",
    "export M2=$M2_HOME/bin\n",
    "export MAVEN_OPTS=\"-Xmx1048m -Xms256m -XX:MaxPermSize=312M\"\n",
    "export PATH=$M2:$PATH\n",
    "\n",
    "```\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <strong>c).</strong> Create <code>GrossVolumeTraining.py script</code>:\n",
    "     ``` bash\n",
    "    [ec2-user@ip-****~]$ cd ./Application/cargoRM/dev/  \n",
    "   [ec2-user@ip-**** dev]$ vim GrossVolumeTraining.py\n",
    "    ```\n",
    "    ---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GrossVolumeTraining.py\n",
    "``` python\n",
    "try:\n",
    "    import h2o\n",
    "    from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "    import random\n",
    "    import sys\n",
    "    import json\n",
    "    import boto3\n",
    "    import logging\n",
    "    from botocore.exceptions import ClientError\n",
    "    from botocore.exceptions import NoCredentialsError\n",
    "    from datetime import date\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"some modules missing {}\".format(e))\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(filename=\"./training_logs.log\",\n",
    "                    level=logging.DEBUG,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s',\n",
    "                    datefmt='%m/%d/%Y %I:%M:%S %p')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "ACCESS_KID = \"\"\n",
    "ACCESS_KEY = \"\"\n",
    "\n",
    "\n",
    "class GrossVolumeTraining(object):\n",
    "    def __init__(self, file_path):\n",
    "        \n",
    "        # start H2O and generated log files, set up the max memory size\n",
    "        h2o.init(log_dir='./h2o_logs', log_level='DEBUG', max_mem_size='1g')  \n",
    "\n",
    "        h2o.remove_all()\n",
    "        \n",
    "        self.split_seed = random.randrange(sys.maxsize)\n",
    "\n",
    "        # Should better use configuration module and configuration files to initialize different training process,\n",
    "        # so that can improve code reusability\n",
    "        \n",
    "        self.file_path = file_path  # input file path\n",
    "        \n",
    "        self.y = 'FinalVolume'  # response column\n",
    "        \n",
    "        self.col_headers = ['Id', 'FinalWeight', 'FinalVolume', 'BookedWeight', 'BookedVolume', 'DepartureDayOfYear',\n",
    "                            'ArrivalDayOfYear', 'DepartureWeek', 'ArrivalWeek', 'DepartureWeekDay', 'ArrivalWeekDay',\n",
    "                            'Origin', 'Destination', 'FlightNumber', 'Suffix', 'Equipment', 'EquipmentInHouse',\n",
    "                            'TransportMode', 'Distance', 'CaptureWeightCapacity', 'CaptureVolumeCapacity', 'LegNumber',\n",
    "                            'Piecese', 'ChargeableWeight', 'Ndo', 'StatusCode', 'PartShipmentIndicator', 'NetCharge',\n",
    "                            'AllotmentCode', 'SpaceAllocationCode', 'Agent', 'POS', 'ProductCode', 'Density', 'Days']\n",
    "\n",
    "        self.col_types = ['numeric', 'numeric', 'numeric', 'numeric', 'numeric', 'numeric', 'numeric', 'numeric',\n",
    "                          'numeric', 'numeric', 'numeric', 'enum', 'enum', 'enum', 'enum', 'enum', 'enum', 'enum',\n",
    "                          'numeric', 'numeric', 'numeric', 'numeric', 'numeric', 'numeric', 'numeric', 'enum',\n",
    "                          'numeric', 'numeric', 'enum', 'enum', 'enum', 'enum', 'enum', 'numeric', 'numeric']\n",
    "\n",
    "        self.gbm_params = {'model_id': 'GrossVolumeModel_gbm',    # POJO target folder\n",
    "                           'nfolds': 0,\n",
    "                           'keep_cross_validation_models': False,\n",
    "                           'keep_cross_validation_predictions': True,\n",
    "                           'keep_cross_validation_fold_assignment': False,\n",
    "                           'score_each_iteration': False,\n",
    "                           'score_tree_interval': 5,\n",
    "                           'fold_assignment': 'AUTO',\n",
    "                           'fold_column': None,\n",
    "                           'response_column': 'FinalVolume',\n",
    "                           'ignored_columns': ['C1',\n",
    "                                               'TransportMode',\n",
    "                                               'ProductCode',\n",
    "                                               'SpaceAllocationCode'],\n",
    "                           'ignore_const_cols': True,\n",
    "                           'offset_column': None,\n",
    "                           'weights_column': None,\n",
    "                           'balance_classes': False,\n",
    "                           'class_sampling_factors': None,\n",
    "                           'max_after_balance_size': 5.0,\n",
    "                           'max_confusion_matrix_size': 20,\n",
    "                           'max_hit_ratio_k': 0,\n",
    "                           'min_rows': 1.0,\n",
    "                           'nbins': 20,\n",
    "                           'nbins_top_level': 1024,\n",
    "                           'nbins_cats': 1024,\n",
    "                           'r2_stopping': 1.7976931348623157e+308,\n",
    "                           'stopping_rounds': 3,\n",
    "                           'stopping_metric': 'RMSE',\n",
    "                           'stopping_tolerance': 0.0010019164953871014,\n",
    "                           'max_runtime_secs': 658812317797974.0,\n",
    "                           'seed': 6785859302972478466,\n",
    "                           'build_tree_one_node': False,\n",
    "                           'learn_rate': 0.05,\n",
    "                           'learn_rate_annealing': 1.0,\n",
    "                           'distribution': 'gaussian',\n",
    "                           'quantile_alpha': 0.5,\n",
    "                           'tweedie_power': 1.5,\n",
    "                           'huber_alpha': 0.9,\n",
    "                           'checkpoint': None,\n",
    "                           'sample_rate': 0.7,\n",
    "                           'sample_rate_per_class': None,\n",
    "                           'col_sample_rate_change_per_level': 1.0,\n",
    "                           'col_sample_rate_per_tree': 0.7,\n",
    "                           'min_split_improvement': 1e-05,\n",
    "                           'histogram_type': 'AUTO',\n",
    "                           'max_abs_leafnode_pred': 1.7976931348623157e+308,\n",
    "                           'pred_noise_bandwidth': 0.0,\n",
    "                           'categorical_encoding': 'AUTO',\n",
    "                           'calibrate_model': False,\n",
    "                           'calibration_frame': None,\n",
    "                           'custom_metric_func': None,\n",
    "                           'custom_distribution_func': None,\n",
    "                           'export_checkpoints_dir': None,\n",
    "                           'monotone_constraints': None,\n",
    "                           'check_constant_response': True,\n",
    "                           'max_depth': 6,\n",
    "                           'ntrees': 1235}\n",
    "\n",
    "    def get_data(self):     # another way is getting input file directly from S3 bucket using boto3\n",
    "        df_raw = h2o.import_file(self.file_path, parse=False)\n",
    "        setup = h2o.parse_setup(df_raw,\n",
    "                                destination_frame=\"training.hex\",\n",
    "                                header=1,\n",
    "                                column_names=self.col_headers,\n",
    "                                column_types=self.col_types)\n",
    "        df = h2o.parse_raw(h2o.parse_setup(df_raw),\n",
    "                           id='training.csv',\n",
    "                           first_line_is_header=1)\n",
    "\n",
    "        logger.info(\"Input dataframe: \", df)\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def split_dataframe(self, df, ratios=[.9], seed=None):   \n",
    "        train, test = df.split_frame(ratios=ratios, seed=seed)\n",
    "        return train, test\n",
    "\n",
    "    def train_gbm(self):\n",
    "        dt_all = self.get_data()\n",
    "\n",
    "        dt_train, dt_test = self.split_dataframe(dt_all, seed=self.split_seed)\n",
    "\n",
    "        y = self.y  # response column\n",
    "\n",
    "        final_gbm = H2OGradientBoostingEstimator(**self.gbm_params)\n",
    "\n",
    "        logger.debug(\"# Start training......\")\n",
    "        final_gbm.train(y=y, training_frame=dt_train, validation_frame=dt_test)\n",
    "\n",
    "        logger.debug('# Downloading model as pojo file......')\n",
    "        final_gbm.download_pojo(self.gbm_params['model_id'])  # POJO target folder\n",
    "\n",
    "        logger.info(\"Final gbm model: \", final_gbm)\n",
    "\n",
    "        logger.debug(\"# Training done.\")\n",
    "\n",
    "        #self.upload_model()\n",
    "\n",
    "    def upload_model(self, dst_bucket='cargo.volume.ml.model', file_name=None):    # I uploaed by AWS CLI in bash script\n",
    "        logger.debug(\"# Uploading model to \", dst_bucket, \" bucket......\")\n",
    "\n",
    "        if file_name is None:\n",
    "            file_name = self.gbm_params['model_id'] + date.today().strftime('%m-%d-%Y')\n",
    "        try:\n",
    "            boto3.setup_default_session(region_name='us-west-2')\n",
    "            s3_client = boto3.client('s3', aws_access_key_id=ACCESS_KID, aws_secret_access_key=ACCESS_KEY)\n",
    "            response = s3_client.upload_file(self.gbm_params['model_id'], dst_bucket, file_name)\n",
    "        except ClientError or NoCredentialsError as ex:\n",
    "            logger.error(ex)\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        raise ValueError(\"VALUE ERROR: Missing training dataset, which must be a csv file.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obj = GrossVolumeTraining(sys.argv[1])\n",
    "    obj.train_gbm()\n",
    "    h2o.shutdown()\n",
    "\n",
    "```\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>\n",
    "    This <code>Application/cargoRM/dev/deploy.sh</code> is not good...  \n",
    "    \n",
    "A good strategy is to put <code>deploy.sh</code> (maybe <code>training.py</code> as well) in a S3 bucket, then download in your \"MLtraining\" lambda function.  \n",
    "    \n",
    "And model deployment command need modification. Refer to \"deploying function by AWS CLI Lambda API Uploading a deployment package with the Lambda API\".\n",
    "</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deploy.sh\n",
    "``` bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Download input file\n",
    "aws s3 cp s3://{DataBucket}/train_volume.csv Application/cargoRM/dev/deploy.sh &&\n",
    "\n",
    "# Training and updating POJO model\n",
    "python Application/cargoRM/dev/GrossVolumeTraining.py Application/cargoRM/dev/data/train_volume.csv &&\n",
    "\n",
    "# Move POJO into maven src folder.\n",
    "sed -i '1s/^/package GrossVolumeModel;/' Application/cargoRM/dev/GrossVolumeModel_gbm/GrossVolumeModel_gbm.java &&\n",
    "cp Application/cargoRM/dev/GrossVolumeModel_gbm/GrossVolumeModel_gbm.java Application/cargoRM/dev/mlModel/src/main/java/GrossVolumeModel/ &&\n",
    "\n",
    "# Maven packaging whole model\n",
    "mvn clean package -f Application/cargoRM/dev/mlModel/pom.xml > Application/cargoRM/dev/mvn_log.txt &&\n",
    "\n",
    "# Uploading model to S3\n",
    "aws s3 cp Application/cargoRM/dev/mlModel/target/mlModel-1.0.0.jar s3://cargo.volume.ml.model/VolumePredictModel/mlModel-1.0.0.1.jar  &&\n",
    "\n",
    "# Uploading traning log to S3\n",
    "zip Application/cargoRM/dev/h2o_logs.zip Application/cargoRM/dev/h2o_logs\n",
    "aws s3 cp Application/cargoRM/dev/h2o_logs.zip s3://{trainingLogBucket}/h2o_logs.zip\n",
    "aws s3 cp Application/cargoRM/dev/mvn_log.txt s3://{trainingLogBucket}/mvn_log.txt\n",
    "aws s3 cp Application/cargoRM/dev/training_logs.txt s3://{trainingLogBucket}/training_logs.txt\n",
    "```\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Tutorial: ML Model Deployment on Valohai</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Install and configure Docker:\n",
    "https://docs.docker.com/get-started/\n",
    "\n",
    "#### 2. Build docker image:\n",
    "([A good tutorial about how to build a machine learning docker image](https://towardsdatascience.com/build-a-docker-container-with-your-machine-learning-model-3cf906f5e07e))\n",
    "\n",
    "Docker official document about how to build and run image: https://docs.docker.com/get-started/part2/\n",
    "\n",
    "Build your image under app root folder. (we only need a runtime evironment to run our training script)\n",
    "```\n",
    "- app-name\n",
    "     |-- requirements.txt  \n",
    "     |-- Dockerfile\n",
    "```\n",
    "\n",
    "##### Dockerfile\n",
    "``` dockerfile\n",
    "FROM python:3.6-stretch\n",
    "MAINTAINER Hamster Potato\n",
    "\n",
    "# install build utilities\n",
    "RUN apt-get update && \\\n",
    "\tapt-get install -y gcc make apt-transport-https ca-certificates build-essential &&\\\n",
    "\tapt-get install -y openjdk-8-jdk && \\\n",
    "\tapt-get install -y ant && \\\n",
    "\tapt-get clean;\n",
    "\n",
    "# Fix certificate issues\n",
    "RUN apt-get update && \\\n",
    "\tapt-get install ca-certificates-java && \\\n",
    "\tapt-get clean && \\\n",
    "\tupdate-ca-certificates -f;\n",
    "\n",
    "# Setup JAVA_HOME -- useful for docker commandline\n",
    "ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/\n",
    "RUN export JAVA_HOME\n",
    "\n",
    "# check our python environment\n",
    "RUN python3 --version\n",
    "RUN pip3 --version\n",
    "\n",
    "# set the working directory for containers\n",
    "WORKDIR  /usr/src/CargoML-img\n",
    "\n",
    "# Installing python dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### requirements.txt\n",
    "\n",
    "```text\n",
    "requests==2.22.0\n",
    "colorama==0.4.3\n",
    "future==0.18.2\n",
    "tabulate >=0.7.5\n",
    "pandas==1.0.1\n",
    "numpy==1.18.1\n",
    "boto3==1.12.39\n",
    "botocore==1.15.39\n",
    "h2o==3.30.0.3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Push Docker image onto Docker Hub:\n",
    "https://docs.docker.com/get-started/part3/\n",
    "\n",
    "Download above image:   (find on: https://hub.docker.com/repository/docker/chenuuu5/cargo-ml-runtime)\n",
    "```shell\n",
    "docker push chenuuu5/cargo-ml-runtime:{tagname}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Install and configure Valohai:\n",
    "https://valohai.com/get-started/?tab=local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Create execution:\n",
    "\n",
    "* <strong>Create Execution</strong>\n",
    "\n",
    "    1. <strong>Settings</strong> --> <strong>Repository</strong> \n",
    "    \n",
    "    Link to your GitHub \"https://github.com/AnnaChenU/AWS-lambda-MLmodel.git \"\n",
    "    \n",
    "    This repository contains traning script (<code>GrossVolumeTraining.py</code>), valohai configuration file (<code>valohai.yaml</code>), model package (<code>{root_folder}</code>)  \n",
    "    \n",
    "    2. <strong>Data</strong> --> <strong>Upload</strong> \n",
    "    \n",
    "    Upload your training data. \n",
    "    \n",
    "    After uploading csv file, copy the <code>datum url</code>, add into your <code>valohai.yml</code> file.\n",
    "    \n",
    "    3. <strong>Execution</strong> --> <strong>Fetch repository</strong> \n",
    "    \n",
    "    Configure execution, based on following <code>valohai.yml</code> file.\n",
    "    \n",
    "    (step2 \"package maven project\" is incorrect... while no question with step1 \"Execute ml\")\n",
    "    \n",
    "##### (Details about valohai configuration: https://docs.valohai.com/valohai-yaml/index.html)\n",
    "    \n",
    "##### valohai.yml\n",
    "```yml\n",
    "- step:\n",
    "    name: Execute ml\n",
    "    image: chenuuu5/cargo-ml-runtime:latest\n",
    "    command: python3 GrossVolumeTraining.py\n",
    "    inputs:\n",
    "      - name: training_sample_input\n",
    "        default: datum://0172fe4a-987e-bbce-1651-b19a23d28f79\n",
    "    environment: aws-eu-west-1-g2-2xlarge\n",
    "\n",
    "\n",
    "- step:\n",
    "    name: package maven project\n",
    "    image: maven\n",
    "    command:\n",
    "      - cp ${VH_INPUTS_DIR}/model/GrossVolumeModel_gbm.java ${VH_REPOSITORY_DIR}/src/main/java/GrossVolumeModel/GrossVolumeModel_gbm.java \n",
    "      - sed -i '1s/^/package GrossVolumeModel;/' ${VH_REPOSITORY_DIR}/src/main/java/GrossVolumeModel/GrossVolumeModel_gbm.java\n",
    "      - mvn clean package -f ${VH_REPOSITORY_DIR}/pom.xml > ${VH_OUTPUTS_DIR}/mvn_log.txt\n",
    "      - cp ${VH_REPOSITORY_DIR}/target/mlModel-1.0.0.jar ${VH_OUTPUTS_DIR}/mlModel-1.0.0.jar\n",
    "    inputs:\n",
    "        - name: model\n",
    "          default: datum://0172fe69-525e-ba45-fb26-af80212c92b0\n",
    "    environment: aws-eu-west-1-g2-2xlarge\n",
    "       \n",
    "- pipeline:\n",
    "    name: Training pipeline\n",
    "    nodes:\n",
    "      - name: execute-ml\n",
    "        type: execution\n",
    "        step: Execute ml\n",
    "      - name: package-jar\n",
    "        type: execution\n",
    "        step: package maven project\n",
    "    edges:\n",
    "      - [execute-ml.output.*, package-jar.input.model]\n",
    "```\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GrossVolumeTraining.py\n",
    "\n",
    "(In order to run on valohai, I do some modification in my training script. Difference is about \"get_data\" and \"dowload_pojo\" part.)\n",
    "``` python\n",
    "try:\n",
    "    import os\n",
    "    import os.path\n",
    "    from os import path\n",
    "    import h2o\n",
    "    from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "    import random\n",
    "    import sys\n",
    "    import json\n",
    "    import boto3\n",
    "    from botocore.exceptions import ClientError\n",
    "    from botocore.exceptions import NoCredentialsError\n",
    "   \n",
    "\n",
    "except Exception as e:\n",
    "    print(\"some modules missing {}\".format(e))\n",
    "\n",
    "INPUT_PATH = os.getenv('VH_INPUTS_DIR', '.inputs/')    # valohai environment variable\n",
    "OUTPUT_PATH = os.getenv('VH_OUTPUTS_DIR', '.outputs/')\n",
    "\n",
    "\n",
    "class GrossVolumeTraining(object):\n",
    "    def __init__(self):\n",
    "        h2o.init(max_mem_size='1G')\n",
    "        h2o.remove_all()\n",
    "        # self.file_path = file_path\n",
    "        self.split_seed = random.randrange(sys.maxsize)\n",
    "        self.y = 'FinalVolume'\n",
    "        self.col_headers = ['C1', 'FinalWeight', 'FinalVolume', 'BookedWeight', 'BookedVolume', 'DepartureDayOfYear',\n",
    "                            'ArrivalDayOfYear', 'DepartureWeek', 'ArrivalWeek', 'DepartureWeekDay', 'ArrivalWeekDay',\n",
    "                            'Origin', 'Destination', 'FlightNumber', 'Suffix', 'Equipment', 'EquipmentInHouse',\n",
    "                            'TransportMode', 'Distance', 'CaptureWeightCapacity', 'CaptureVolumeCapacity', 'LegNumber',\n",
    "                            'Piecese', 'ChargeableWeight', 'Ndo', 'StatusCode', 'PartShipmentIndicator', 'NetCharge',\n",
    "                            'AllotmentCode', 'SpaceAllocationCode', 'Agent', 'POS', 'ProductCode', 'Density', 'Days']\n",
    "        self.col_types = ['numeric', 'numeric', 'numeric', 'numeric', 'numeric', 'numeric', 'numeric', 'numeric',\n",
    "                          'numeric', 'numeric', 'numeric', 'enum', 'enum', 'enum', 'enum', 'enum', 'enum', 'enum',\n",
    "                          'numeric', 'numeric', 'numeric', 'numeric', 'numeric', 'numeric', 'numeric', 'enum',\n",
    "                          'numeric', 'numeric', 'enum', 'enum', 'enum', 'enum', 'enum', 'numeric', 'numeric']\n",
    "        self.gbm_params = {'model_id': 'GrossVolumeModel_gbm',\n",
    "                           'nfolds': 0,\n",
    "                           'keep_cross_validation_models': False,\n",
    "                           'keep_cross_validation_predictions': True,\n",
    "                           'keep_cross_validation_fold_assignment': False,\n",
    "                           'score_each_iteration': False,\n",
    "                           'score_tree_interval': 5,\n",
    "                           'fold_assignment': 'AUTO',\n",
    "                           'fold_column': None,\n",
    "                           'response_column': 'FinalVolume',\n",
    "                           'ignored_columns': ['C1',\n",
    "                                               'TransportMode',\n",
    "                                               'ProductCode',\n",
    "                                               'SpaceAllocationCode'],\n",
    "                           'ignore_const_cols': True,\n",
    "                           'offset_column': None,\n",
    "                           'weights_column': None,\n",
    "                           'balance_classes': False,\n",
    "                           'class_sampling_factors': None,\n",
    "                           'max_after_balance_size': 5.0,\n",
    "                           'max_confusion_matrix_size': 20,\n",
    "                           'max_hit_ratio_k': 0,\n",
    "                           'min_rows': 1.0,\n",
    "                           'nbins': 20,\n",
    "                           'nbins_top_level': 1024,\n",
    "                           'nbins_cats': 1024,\n",
    "                           'r2_stopping': 1.7976931348623157e+308,\n",
    "                           'stopping_rounds': 3,\n",
    "                           'stopping_metric': 'RMSE',\n",
    "                           'stopping_tolerance': 0.0010019164953871014,\n",
    "                           'max_runtime_secs': 658812317797974.0,\n",
    "                           'seed': 6785859302972478466,\n",
    "                           'build_tree_one_node': False,\n",
    "                           'learn_rate': 0.05,\n",
    "                           'learn_rate_annealing': 1.0,\n",
    "                           'distribution': 'gaussian',\n",
    "                           'quantile_alpha': 0.5,\n",
    "                           'tweedie_power': 1.5,\n",
    "                           'huber_alpha': 0.9,\n",
    "                           'checkpoint': None,\n",
    "                           'sample_rate': 0.7,\n",
    "                           'sample_rate_per_class': None,\n",
    "                           'col_sample_rate_change_per_level': 1.0,\n",
    "                           'col_sample_rate_per_tree': 0.7,\n",
    "                           'min_split_improvement': 1e-05,\n",
    "                           'histogram_type': 'AUTO',\n",
    "                           'max_abs_leafnode_pred': 1.7976931348623157e+308,\n",
    "                           'pred_noise_bandwidth': 0.0,\n",
    "                           'categorical_encoding': 'AUTO',\n",
    "                           'calibrate_model': False,\n",
    "                           'calibration_frame': None,\n",
    "                           'custom_metric_func': None,\n",
    "                           'custom_distribution_func': None,\n",
    "                           'export_checkpoints_dir': None,\n",
    "                           'monotone_constraints': None,\n",
    "                           'check_constant_response': True,\n",
    "                           'max_depth': 6,\n",
    "                           'ntrees': 1235}\n",
    "\n",
    "        \n",
    "    # I'm not sure if it can directly download input csv file by boto3 and store in {VH_INPUTS_DIR}  \n",
    "    def get_data(self, src_bucket=\"cargo.ml.training\", obj_name=\"training_sample.csv\"):\n",
    "        # boto3.setup_default_session(region_name='us-west-2')\n",
    "        # s3_client = boto3.client('s3', aws_access_key_id=ACCESS_KID, aws_secret_access_key=ACCESS_KEY)\n",
    "        input_path = os.path.join(INPUT_PATH, 'training_sample_input/training_sample.csv')\n",
    "        # s3_client.download_file(src_bucket, obj_name, input_path)\n",
    "\n",
    "        df_raw = h2o.import_file(input_path, parse=False)\n",
    "        setup = h2o.parse_setup(df_raw,\n",
    "                                destination_frame=\"training.hex\",\n",
    "                                header=1,\n",
    "                                column_names=self.col_headers,\n",
    "                                column_types=self.col_types)\n",
    "        df = h2o.parse_raw(h2o.parse_setup(df_raw),\n",
    "                           id='training.csv',\n",
    "                           first_line_is_header=1)\n",
    "        print(\"Input dataframe: \", df)\n",
    "        return df\n",
    "\n",
    "    def split_dataframe(self, df, ratios=[.9], seed=None):\n",
    "        train, test = df.split_frame(ratios=ratios, seed=seed)\n",
    "        return train, test\n",
    "\n",
    "    def train_gbm(self):\n",
    "        dt_all = self.get_data()\n",
    "        dt_train, dt_test = self.split_dataframe(dt_all, seed=self.split_seed)\n",
    "        y = self.y  # response column\n",
    "        final_gbm = H2OGradientBoostingEstimator(**self.gbm_params)\n",
    "        print(\"# Start training......\")\n",
    "        final_gbm.train(y=y, training_frame=dt_train, validation_frame=dt_test)\n",
    "        print('# Downloading model as pojo file......')\n",
    "        try:\n",
    "            print(\"Dowloaing pojo.\")\n",
    "            final_gbm.download_pojo(os.path.join(OUTPUT_PATH, 'gbm_model'))\n",
    "            if path.exists(os.path.join(OUTPUT_PATH, 'gbm_model.java')):\n",
    "                print(\"Pojo downloaded successfully\")\n",
    "            else:\n",
    "                print(\"No pojo!!!!!!!!!\")\n",
    "        except Exception as ex:\n",
    "            print(\"POJO downloading error: {}\".format(ex))\n",
    "        print(final_gbm)\n",
    "        print(\"# Training done.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Start!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obj = GrossVolumeTraining()\n",
    "    obj.train_gbm()\n",
    "    h2o.shutdown()\n",
    "```\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
